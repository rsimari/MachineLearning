{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    homework3 for Machine Learning \n",
    "    author: Robert Simari <rsimari>\n",
    "    date: 2/13/18\n",
    "    purpose: builds a decision tree based on heart data\n",
    "\"\"\"\n",
    "\n",
    "from math import log\n",
    "import graphviz as gv\n",
    "\n",
    "def compute_entropy(data):\n",
    "    \"\"\"\n",
    "        @param [list[list]]: 2D array of data\n",
    "        @return [float]: entropy of given data\n",
    "        Note: assumes that the outcome is the final column in each list\n",
    "        entropy(Y) = -p_1 * log_2(p_1) - p_2 * log_2(p_2) - ...\n",
    "    \"\"\"\n",
    "    total_instances = len(data)\n",
    "    outcome_index = len(data[0]) - 1\n",
    "    \n",
    "    # all unique outcomes possible, assumes nominal\n",
    "    outcomes = list(set([arr[outcome_index] for arr in data]))\n",
    "    \n",
    "    # gives counts for all the outcomes possible for a particular set of instances with a feature value\n",
    "    outcome_count = {}\n",
    "    for arr in data:\n",
    "        if arr[outcome_index] in outcome_count:\n",
    "            outcome_count[arr[outcome_index]] += 1\n",
    "        else:\n",
    "            outcome_count[arr[outcome_index]] = 1\n",
    "            \n",
    "    # computes entropy sum using each outcome possible (assuming nominal outcomes)\n",
    "    entropy = 0\n",
    "    for _, count in outcome_count.items():\n",
    "        r = count * 1.0 / total_instances\n",
    "        entropy = entropy - r * log(r, 2)\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def compute_cond_entropy(feature_name, data, split_func = None):\n",
    "    \"\"\"\n",
    "        @param [string, string, list[list], function]: name of feature (see feature_names list), 2D list of values, an optional\n",
    "        function to split a continuous feature into bins\n",
    "        @return [float]: returns conditional entropy value of Y given X\n",
    "        H(Y|X) = sum(P(x_k = v_j) * H(Y|x_k = v_j)) \n",
    "                            or\n",
    "        entropy(Y | x_k) = for each x_k: sum((# of instances of x_k)/(# of instances) * entropy(x_k, x_k instances))\n",
    "    \"\"\"\n",
    "    # to compute entropy(outcome, x_k instances):\n",
    "    #     1. select all cases with x_k, call it data2\n",
    "    #     2. return compute_entropy(x_k, data2)\n",
    "    if feature_name in feature_names:\n",
    "        index = feature_names.index(feature_name)\n",
    "    else:\n",
    "        return -1 # feature is unknown\n",
    "    \n",
    "    total_instances = len(data)\n",
    "    cond_entropy = 0\n",
    "\n",
    "    # getting all possible values/bins\n",
    "    if split_func == None:\n",
    "        bins = list(set([arr[index] for arr in data]))\n",
    "    else:\n",
    "        bins = list(set(split_func(arr[index]) for arr in data)) # split function accounts for continuous features\n",
    "    \n",
    "    # for every feature value/bin\n",
    "    for value in bins:\n",
    "        # compute P(x_k = v_j)\n",
    "        if split_func == None:\n",
    "            feature_instances = [arr for arr in data if arr[index] == value] # nominal, instances that have our desired value\n",
    "        else:\n",
    "            feature_instances = [arr for arr in data if split_func(arr[index]) == value] # continuous, count instances of x_k\n",
    "        count = len(feature_instances)\n",
    "        prob = count * 1.0 / total_instances\n",
    "        \n",
    "        # compute H(Y|x_k = v_j)\n",
    "        entropy = compute_entropy(feature_instances)\n",
    "        \n",
    "        # add to sum for conditional entropy\n",
    "        cond_entropy += (entropy * prob)\n",
    "    \n",
    "    return cond_entropy\n",
    "\n",
    "def compute_inf_gain(feature_name, data, split_func = None):\n",
    "    \"\"\"\n",
    "        @param [string, list[list], function]: name of column for feature, 2D list of data, function for splitting\n",
    "        continuous features\n",
    "        @return [float]: information gain of Y given X\n",
    "        infGain(Y | x_k) = entropy(Y) - entropy(Y | x_k)\n",
    "    \"\"\"\n",
    "    return compute_entropy(data) - compute_cond_entropy(feature_name, data, split_func)\n",
    "\n",
    "def compute_max_inf_gain(data, features):\n",
    "    \"\"\"\n",
    "        @param [list[list], list]: 2D list of data, the list of features to compare\n",
    "        @return [string, float]: name of feature, information gain of feature\n",
    "    \"\"\"\n",
    "    mx = -1\n",
    "    for feature in features:\n",
    "        # check if split function is needed?\n",
    "        ig = compute_inf_gain(feature, data)\n",
    "        mx = max(mx, ig)\n",
    "        if ig == mx: \n",
    "            mx_feature = feature\n",
    "    return mx_feature, mx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split functions for each feature\n",
    "\n",
    "def split_nominal(value, feature): # organizes the nominal features into discrete bins\n",
    "    feature_index = feature_names.index(feature)\n",
    "    all_vals = list(set([arr[feature_index] for arr in data]))\n",
    "    return all_vals.index(value)\n",
    "\n",
    "def split_age(age):\n",
    "    if float(age) > 50:\n",
    "        return 1 # returns a bin number for an age to be place in\n",
    "    return 0\n",
    "\n",
    "def split_sex(sex):\n",
    "    return split_nominal(sex, \"sex\")\n",
    "\n",
    "def split_chest(chest):\n",
    "    return split_nominal(chest, \"chest\")\n",
    "\n",
    "def split_rbp(rbp):\n",
    "    if float(rbp) > 130:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def split_sc(sc):\n",
    "    if float(sc) > 250:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def split_fbs(fbs):\n",
    "    return split_nominal(fbs, \"fbs\")\n",
    "\n",
    "def split_rer(rer):\n",
    "    return split_nominal(rer, \"rer\")\n",
    "\n",
    "def split_maxhr(maxhr):\n",
    "    if float(maxhr) > 120:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def split_eia(eia):\n",
    "    return split_nominal(eia, \"eia\")\n",
    "\n",
    "def split_oldpeak(oldpeak):\n",
    "    if float(oldpeak) > 1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def split_slopest(slopest):\n",
    "    return split_nominal(slopest, \"slopest\")\n",
    "\n",
    "def split_vessels(vessels):\n",
    "    return split_nominal(vessels, \"vessels\")\n",
    "\n",
    "def split_thal(thal):\n",
    "    return split_nominal(thal, \"thal\")\n",
    "\n",
    "# dict maps each feature to its split function\n",
    "split_dict = {\n",
    "    \"age\" : split_age,\n",
    "    \"sex\" : split_sex, \n",
    "    \"chest\": split_chest, \n",
    "    \"rbp\" : split_rbp, \n",
    "    \"sc\" : split_sc, \n",
    "    \"fbs\" : split_fbs, \n",
    "    \"rer\" : split_rer, \n",
    "    \"maxhr\" : split_maxhr, \n",
    "    \"eia\" : split_eia, \n",
    "    \"oldpeak\" : split_oldpeak, \n",
    "    \"slopest\" : split_slopest, \n",
    "    \"vessels\" : split_vessels,\n",
    "    \"thal\" : split_thal  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Decision Tree Using Information Gain\n",
    "\n",
    "class Decision_Node(object): # possibly a wrapper for the graphix pkg node\n",
    "    def __init__(self):\n",
    "        self.children = []\n",
    "        self.split_feature = \"\"\n",
    "        \n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "        \n",
    "class Decision_Tree(object):\n",
    "    def __init__(self, data, feature_names):\n",
    "        self.root = Decision_Node()\n",
    "        self.features = feature_names\n",
    "        self.data = data\n",
    "    \n",
    "    def build_tree(self):\n",
    "        if len(self.data) == 0:\n",
    "            return\n",
    "        self.build_next_split(self.root, self.features, self.data)\n",
    "    \n",
    "    def build_next_split(self, root, features, data):\n",
    "        if len(features) == 0 or len(data) == 0:\n",
    "            return\n",
    "\n",
    "        feature, inf_gain = compute_max_inf_gain(data, features)\n",
    "        # get index of feature in data \n",
    "        feature_index = feature_names.index(feature)\n",
    "        # get index of feature in local feature list (features)\n",
    "        feature_index2 = features.index(feature)\n",
    "        # set split by feature for the current node\n",
    "        root.split_feature = feature\n",
    "        # create children, based on split functions\n",
    "        split_func = split_dict[feature]\n",
    "        bins = list(set([split_func(arr[feature_index]) for arr in data]))\n",
    "        for b in bins:\n",
    "            child = Decision_Node()\n",
    "            root.add_child(child)\n",
    "            # filter data that only has the desired feature values/bin of the branch\n",
    "            data2 = [arr for arr in data if split_func(arr[feature_index]) == b]\n",
    "            # recurse on each branch with the filtered feature list \n",
    "            self.build_next_split(child, features[:feature_index2] + features[feature_index2+1:], data2)\n",
    "\n",
    "        # remove feature from being in future splits (assumes that continuous features only split once)\n",
    "        features.remove(feature)\n",
    "        return\n",
    "    \n",
    "    def make_prediction(self, data):\n",
    "        pass\n",
    "    \n",
    "    def view(self, file = 'dt.gv'):\n",
    "        graph = gv.Digraph('DecisionTree', filename = file)\n",
    "        q = []\n",
    "        q.insert(0, self.root)\n",
    "        while len(q) != 0:\n",
    "            node = q.pop()\n",
    "            for child in node.children:\n",
    "                graph.edge(node.split_feature, child.split_feature)\n",
    "                q.insert(0, child)\n",
    "        graph.view()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"age\", \"sex\", \"chest\", \"rbp\", \"sc\", \"fbs\", \"rer\", \"maxhr\", \"eia\", \"oldpeak\", \"slopest\", \"vessels\", \"thal\"]\n",
    "\n",
    "TEST_MODE = False\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(\"heart.data\") as f:\n",
    "    line = f.readline().rstrip()\n",
    "    while line:\n",
    "        cols = line.split()\n",
    "        data.append(cols)\n",
    "        line = f.readline().rstrip()\n",
    "\n",
    "if TEST_MODE:\n",
    "    feature_names = ['school']\n",
    "    test = [['ND', 'YES'], ['MSU', 'NO'], ['ND', 'NO'], ['ND', 'YES'], ['ND', 'NO'], ['USC', 'YES'], ['MSU', 'NO'], ['USC', 'YES']]\n",
    "    assert(compute_entropy(test) == 1)\n",
    "    assert(compute_cond_entropy('school', test) == 0.5)\n",
    "    assert(compute_max_inf_gain(test) == ('school', 0.5))\n",
    "    t\n",
    "tree = Decision_Tree(data, feature_names)\n",
    "tree.build_tree()\n",
    "# assert(tree.root.split_feature == 'sc')\n",
    "# tree.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
